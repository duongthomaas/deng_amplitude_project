# Amplitude Data Extraction & Processing

This project contains a pipeline of Python scripts designed to extract raw event data from the Amplitude API and process the compressed archives into clean, usable JSON files ready for cloud storage (AWS S3 & Snowflake).

## Scripts Overview

### 1. Data Extractor (`extract_amplitude.py`)

Connects to the Amplitude Export API to download event data for a specific date range.

- **Input:** Amplitude API Key and Secret.
- **Output:** A raw compressed archive named `data.zip`.

### 2. Data Unzipper (`unzip_amplitude.py`)

Handles the "Russian Doll" compression structure of Amplitude exports (a `.zip` file containing nested folders of `.gz` files).

- **Input:** The `data.zip` file.
- **Process:**
  1.  Extracts `data.zip` to a temporary workspace.
  2.  Goes through the extracted folders to find all `.gz` files.
  3.  Decompresses each `.gz` file into a clean `.json` file.
  4.  Consolidates all `.json` files into a single `data/` directory.
  5.  Cleans up temporary files.
- **Output:** A `data/` folder containing raw JSON event logs.

## üõ†Ô∏è Prerequisites

- Python 3.x
- No external libraries required for the unzip script (uses standard library `zipfile`, `gzip`, `shutil`, `os`).
- `requests` library required for the extractor script.

---

# Data Staging & Loading

The following steps outline the architecture used to move the processed data from the local environment into S3 Bucket and finally into Snowflake.

## 3. S3 Bucket

To stage the data securely, an S3 bucket was configured with security measures.

- **Bucket Structure:**
  - `python-import/`: Designated folder for JSON files uploaded via the Python script.
- **Security Configuration:**
  - **Public Access:** Blocked entirely (Block Public Access settings enabled).
  - **Encryption:** Server-side encryption enabled using a custom **AWS KMS Key**.
  - **Ownership:** ACLs disabled; bucket owner enforced.

## 4. Connectivity & Access Control (IAM)

Two distinct security methods were used to connect to the S3 bucket, ensuring the principle of least privilege.

### Connection A: Python Script ‚Üí S3 (Write Access)

This connection is used by the `load_data.py` script to upload files.

- **Identity Type:** **IAM User** (`amplitude_python_uploader`)
- **Authentication:** Programmatic Access via **Access Key ID** & **Secret Access Key** (stored in `.env`).
- **Permissions:** attached via an **IAM Policy** allowing `s3:PutObject` specifically for the `python-import/` folder.

### Connection B: Snowflake ‚Üí S3 (Read Access)

This connection allows Snowflake to ingest data without exchanging access keys.

- **Identity Type:** **IAM Role** (`snowflake-amplitude-python`)
- **Authentication:** **Trusted Relationship** (STS AssumeRole).
- **Mechanism:**
  1.  An **External ID** is generated by Snowflake.
  2.  The AWS IAM Role is configured with a **Trust Policy** that only allows the specific Snowflake user (referenced by ARN) to assume the role if the External ID matches.
- **Permissions:** The Role has an attached policy allowing `s3:GetObject` and `s3:ListBucket`.

## 5. Snowflake

Once the data is staged in S3, the following objects were created in Snowflake to finalise the load.

1.  **Storage Integration:** An object that stores the AWS IAM Role ARN, acting as the secure handshake between Snowflake and AWS.
2.  **External Stage:** A pointer that references the Storage Integration and the specific S3 URL (`s3://.../python-import/`).
3.  **File Format:** Defined as `JSON` with `STRIP_OUTER_ARRAY = TRUE` to handle the data structure.
4.  **Data Loading:**
    - **Target Table:** `amplitude_raw` (uses a `VARIANT` column to store raw JSON).
    - **Command:** `COPY INTO` is executed to move data from the External Stage into the Target Table.
