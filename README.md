# Amplitude Data Extraction & Load

This project contains a pipeline of Python scripts designed to extract raw event data from the Amplitude API and process the compressed archives into clean, usable JSON files ready for cloud storage (AWS S3 & Snowflake).

## Extracting and Unzipping

### 1. Data Extractor (`extract_amplitude.py`)

Connects to the Amplitude Export API to download event data for a specific date range.

- **Input:** Amplitude API Key and Secret.
- **Output:** A raw compressed archive named `data.zip`.

### 2. Data Unzipper (`unzip_amplitude.py`)

Handles the "Russian Doll" compression structure of Amplitude exports (a `.zip` file containing nested folders of `.gz` files).

- **Input:** The `data.zip` file.
- **Process:**
  1.  Extracts `data.zip` to a temporary workspace.
  2.  Goes through the extracted folders to find all `.gz` files.
  3.  Decompresses each `.gz` file into a clean `.json` file.
  4.  Consolidates all `.json` files into a single `data/` directory.
  5.  Cleans up temporary files.
- **Output:** A `data/` folder containing raw JSON event logs.

## Staging & Loading

The following steps outline the architecture used to move the processed data from the local environment into S3 Bucket and finally into Snowflake.

<img width="1535" height="987" alt="image" src="https://github.com/user-attachments/assets/baa01247-18dd-4fc4-bbe4-d9740ad624e3" />

## 3. S3 Bucket

To stage the data securely, an S3 bucket was configured with security measures.

- **Bucket Structure:**
  - `python-import/`: Designated folder for JSON files uploaded via the Python script.
- **Security Configuration:**
  - **Public Access:** Blocked entirely (Block Public Access settings enabled).
  - **Encryption:** Server-side encryption enabled using a custom **AWS KMS Key**.
  - **Ownership:** ACLs disabled; bucket owner enforced.

## 4. Connectivity & Access Control (IAM)

Two distinct security methods were used to connect to the S3 bucket, ensuring the principle of least privilege.

### Connection A: Python Script ‚Üí S3 (Write Access)

This connection is used by the `load_data.py` script to upload files.

- **Identity Type:** **IAM User** (`amplitude_python_uploader`)
- **Authentication:** Programmatic Access via **Access Key ID** & **Secret Access Key** (stored in `.env`).
- **Permissions:** attached via an **IAM Policy** allowing `s3:PutObject` specifically for the `python-import/` folder.

### Connection B: Snowflake ‚Üí S3 (Read Access)

This connection allows Snowflake to ingest data without exchanging access keys.

- **Identity Type:** **IAM Role** (`snowflake-amplitude-python`)
- **Authentication:** **Trusted Relationship** (STS AssumeRole).
- **Mechanism:**
  1.  An **External ID** is generated by Snowflake.
  2.  The AWS IAM Role is configured with a **Trust Policy** that only allows the specific Snowflake user (referenced by ARN) to assume the role if the External ID matches.
- **Permissions:** The Role has an attached policy allowing `s3:GetObject` and `s3:ListBucket`.

## 5. Snowflake

Once the data is staged in S3, the following objects were created in Snowflake to finalise the load.

1.  **Storage Integration:** An object that stores the AWS IAM Role ARN, acting as the secure handshake between Snowflake and AWS.
2.  **External Stage:** A pointer that references the Storage Integration and the specific S3 URL (`s3://.../python-import/`).
3.  **File Format:** Defined as `JSON` with `STRIP_OUTER_ARRAY = TRUE` to handle the data structure.
4.  **Data Loading:**
    - **Target Table:** `amplitude_raw` (uses a `VARIANT` column to store raw JSON).
    - **Command:** `COPY INTO` is executed to move data from the External Stage into the Target Table.

## üöÄ Pipeline Execution

The entire pipeline has been modularised and is controlled by a single entry point: `main.py`.

### Master Orchestrator (`main.py`)

This script integrates all pipeline stages (Extraction, Processing, Loading) into a single flow.

- **Function:**
  1.  Calculates the date range (defaults to "Yesterday" for daily automation).
  2.  Initialises logging.
  3.  Triggers the **Extraction Module**.
  4.  Triggers the **Unzip/Process Module**.
  5.  Triggers the **Load Module** to upload to S3.

## ü§ñ Automation & Orchestration

While main.py can be run locally, this pipeline is designed for automation.

### Kestra Integration

This Python workflow can be orchestrated using Kestra.

`Trigger`: Schedule the main.py script to run daily (e.g., at 01:00 UTC) to fetch the previous day's data.

`Workflow`: Kestra can manage the execution environment, handle retries on API failures, and trigger downstream DBT models in Snowflake once the Python load is complete.

## üõ†Ô∏è Prerequisites

- Python 3.x
- No external libraries required for the unzip script (uses standard library `zipfile`, `gzip`, `shutil`, `os`).
- `requests` library required for the extractor script.
- **Environment Variables:**
  You must create a `.env` file in the root directory containing the following keys (matching the `main.py` configuration):
  ```ini
  AMP_API_KEY=your_amplitude_api_key
  AMP_SECRET_KEY=your_amplitude_secret_key
  AWS_ACCESS_KEY=your_aws_access_key
  AWS_SECRET_KEY=your_aws_secret_key
  bucket_name=your_s3_bucket_name
  ```
